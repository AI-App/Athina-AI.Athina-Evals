{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "from athina.evals import DoesResponseAnswerQuery, ContextContainsEnoughInformation, Faithfulness, LlmEvaluator\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shooting star, also known as a meteor, is not related to black holes. It is a small celestial object, such as a rock or dust particle, that enters the Earth's atmosphere and burns up due to friction with the air. This creates a streak of light in the sky, which is commonly referred to as a shooting star.\n"
     ]
    }
   ],
   "source": [
    "# Your inference call to OpenAI\n",
    "model = \"gpt-3.5-turbo\"\n",
    "user_query = \"What is a shooting star?\"\n",
    "context = \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Use the information provided to you to answer the user's question. Information: {context}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_query\n",
    "    }\n",
    "]\n",
    "\n",
    "openai_service = OpenAiService()\n",
    "response = openai_service.chat_completion(prompt, model=model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure': False,\n",
       " 'reason': \"The response answers the user's query by explaining what a shooting star is, also clarifying that it is known as a meteor and is not related to black holes. It covers the aspects of the user's query by describing the nature of a shooting star as a small celestial object that burns up in the Earth's atmosphere, creating a visible streak of light.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the answer relevance evaluator\n",
    "# Checks if the LLM response answers the user query sufficiently\n",
    "DoesResponseAnswerQuery().run(user_query=user_query, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure': True,\n",
       " 'reason': 'The response cannot be inferred from the provided context. The context describes black holes and their properties, but it does not provide any information about shooting stars or meteors, nor does it make any comparison between black holes and other celestial phenomena.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the faithfulness evaluator\n",
    "# Checks if the LLM response is faithful to the information provided to it\n",
    "Faithfulness().run(context=context, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure': True,\n",
       " 'reason': \"The context provided talks about black holes, not shooting stars. Therefore, it does not contain sufficient information to answer the user's query about what a shooting star is.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the ContextContainsEnoughInformation evaluator\n",
    "# Checks if the context contains enough information to answer the user query provided\n",
    "ContextContainsEnoughInformation(model=\"gpt-4\").run(context=context, user_query=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure': True,\n",
       " 'reason': 'The response mentions black holes, which according to the grading criteria, results in a fail.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the context relevance evaluator\n",
    "# Checks if the context is relevant to the user query\n",
    "grading_criteria=\"If the response mentions black holes, then fail. Otherwise pass.\"\n",
    "LlmEvaluator(grading_criteria=grading_criteria).run(context=context, user_query=user_query, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
