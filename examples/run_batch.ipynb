{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "from athina.evals import DoesResponseAnswerQuery, ContextContainsEnoughInformation, Faithfulness, CustomGrader\n",
    "from athina.loaders import RagLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.interfaces.athina import AthinaFilters\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch dataset from list of dict objects\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of Greece?\",\n",
    "        \"context\": \"Greece is often called the cradle of Western civilization.\",\n",
    "        \"response\": \"Athens\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the price of a Tesla Model 3?\",\n",
    "        \"context\": \"Tesla Model 3 is a fully electric car.\",\n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a shooting star?\",\n",
    "        \"context\": \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\",\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'What is the capital of Greece?', 'context': 'Greece is often called the cradle of Western civilization.', 'response': 'Athens'}, {'query': 'What is the price of a Tesla Model 3?', 'context': 'Tesla Model 3 is a fully electric car.', 'response': 'I cannot answer this question as prices vary from country to country.'}]\n"
     ]
    }
   ],
   "source": [
    "# Your inference call to OpenAI\n",
    "model = \"gpt-3.5-turbo\"\n",
    "eval_model = \"gpt-4-1106-preview\"\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'DoesResponseAnswerQuery',\n",
       "  'data': {'query': 'What is the capital of Greece?',\n",
       "   'context': 'Greece is often called the cradle of Western civilization.',\n",
       "   'response': 'Athens'},\n",
       "  'failure': False,\n",
       "  'reason': \"The response 'Athens' directly answers the user's query about the capital of Greece. It is specific and covers the aspect of the user's query that asks for the name of the capital.\",\n",
       "  'runtime': 3801,\n",
       "  'model': 'gpt-4-1106-preview'},\n",
       " {'name': 'DoesResponseAnswerQuery',\n",
       "  'data': {'query': 'What is the price of a Tesla Model 3?',\n",
       "   'context': 'Tesla Model 3 is a fully electric car.',\n",
       "   'response': 'I cannot answer this question as prices vary from country to country.'},\n",
       "  'failure': True,\n",
       "  'reason': \"The response does not answer the user's query sufficiently. It does not provide any specific price information or a range for the Tesla Model 3, nor does it offer to provide a source where the user could find the price. Instead, it only states that prices vary, which does not fulfill the user's request for price information.\",\n",
       "  'runtime': 4960,\n",
       "  'model': 'gpt-4-1106-preview'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the LLM response answers the user query sufficiently\n",
    "DoesResponseAnswerQuery(model=eval_model).run_batch(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the LLM response is faithful to the information provided to it\n",
    "Faithfulness(model=eval_model).run_batch(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the context contains enough information to answer the user query provided\n",
    "ContextContainsEnoughInformation(model=eval_model).run_batch(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom evaluator\n",
    "# Checks if the response mentions black holes\n",
    "grading_criteria=\"If the response mentions black holes, then fail. Otherwise pass.\"\n",
    "CustomGrader(model=eval_model, grading_criteria=grading_criteria).run_batch(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
