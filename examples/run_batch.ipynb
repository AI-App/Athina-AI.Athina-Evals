{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "from athina.evals import DoesResponseAnswerQuery, ContextContainsEnoughInformation, Faithfulness, CustomGrader\n",
    "from athina.loaders import RagLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.interfaces.athina import AthinaFilters\n",
    "import pandas as pd\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western c...</td>\n",
       "      <td>Athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed unde...</td>\n",
       "      <td>A shooting star is a meteor that burns up in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Greece is often called the cradle of Western c...   \n",
       "1             Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed unde...   \n",
       "\n",
       "                                            response  \n",
       "0                                             Athens  \n",
       "1  I cannot answer this question as prices vary f...  \n",
       "2  A shooting star is a meteor that burns up in t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batch dataset from list of dict objects\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of Greece?\",\n",
    "        \"context\": \"Greece is often called the cradle of Western civilization.\",\n",
    "        \"response\": \"Athens\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the price of a Tesla Model 3?\",\n",
    "        \"context\": \"Tesla Model 3 is a fully electric car.\",\n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a shooting star?\",\n",
    "        \"context\": \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\",\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response 'Athens' answers specifically what the user is asking about, which is the capital of Greece. It covers all aspects of the user's query and provides the correct answer.</td>\n",
       "      <td>1790</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response sufficiently answers the user's query. It acknowledges that the prices of Tesla Model 3 vary from country to country, which directly addresses the user's question about the price. Therefore, the response covers all aspects of the user's query and provides a satisfactory answer.</td>\n",
       "      <td>2157</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response answers the user's query sufficiently. It provides a clear and concise definition of a shooting star, explaining that it is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>2188</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "                 display_name  failed  \\\n",
       "0  Does Response Answer Query   False   \n",
       "1  Does Response Answer Query   False   \n",
       "2  Does Response Answer Query   False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                          grade_reason  \\\n",
       "0                                                                                                                 The response 'Athens' answers specifically what the user is asking about, which is the capital of Greece. It covers all aspects of the user's query and provides the correct answer.   \n",
       "1  The response sufficiently answers the user's query. It acknowledges that the prices of Tesla Model 3 vary from country to country, which directly addresses the user's question about the price. Therefore, the response covers all aspects of the user's query and provides a satisfactory answer.   \n",
       "2                                                                                                                   The response answers the user's query sufficiently. It provides a clear and concise definition of a shooting star, explaining that it is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     1790  gpt-3.5-turbo    failed           0.0  \n",
       "1     2157  gpt-3.5-turbo    failed           0.0  \n",
       "2     2188  gpt-3.5-turbo    failed           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the LLM response answers the user query sufficiently\n",
    "eval_model = \"gpt-3.5-turbo\"\n",
    "DoesResponseAnswerQuery(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>True</td>\n",
       "      <td>The response 'Athens' cannot be inferred purely from the context. The context mentions that Greece is often called the cradle of Western civilization, but it does not directly state that Athens is the cradle of Western civilization.</td>\n",
       "      <td>2010</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>False</td>\n",
       "      <td>The response can be inferred from the context. The context states that the prices of Tesla Model 3 vary from country to country, which implies that the chatbot cannot provide a specific answer to the question about prices.</td>\n",
       "      <td>2151</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>True</td>\n",
       "      <td>The response cannot be inferred from the provided context. The context talks about black holes and their gravitational pull, while the response talks about shooting stars and their burning up in the atmosphere. There is no connection or mention of shooting stars in the context, so the response cannot be inferred purely from the context.</td>\n",
       "      <td>2613</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "   display_name  failed  \\\n",
       "0  Faithfulness    True   \n",
       "1  Faithfulness   False   \n",
       "2  Faithfulness    True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                         grade_reason  \\\n",
       "0                                                                                                            The response 'Athens' cannot be inferred purely from the context. The context mentions that Greece is often called the cradle of Western civilization, but it does not directly state that Athens is the cradle of Western civilization.   \n",
       "1                                                                                                                      The response can be inferred from the context. The context states that the prices of Tesla Model 3 vary from country to country, which implies that the chatbot cannot provide a specific answer to the question about prices.   \n",
       "2  The response cannot be inferred from the provided context. The context talks about black holes and their gravitational pull, while the response talks about shooting stars and their burning up in the atmosphere. There is no connection or mention of shooting stars in the context, so the response cannot be inferred purely from the context.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     2010  gpt-3.5-turbo    failed           1.0  \n",
       "1     2151  gpt-3.5-turbo    failed           0.0  \n",
       "2     2613  gpt-3.5-turbo    failed           1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the LLM response is faithful to the information provided to it\n",
    "Faithfulness(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context provided does not contain sufficient information to answer the user's query about the capital of Greece. The context only mentions that Greece is often called the cradle of Western civilization, but it does not provide any specific information about the capital city.</td>\n",
       "      <td>2183</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context provided does not contain sufficient information to answer the user's query about the price of a Tesla Model 3. The context only mentions that Tesla Model 3 is a fully electric car, but it does not provide any specific information about its price.</td>\n",
       "      <td>2078</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context provided does not contain any information related to shooting stars. Therefore, the chatbot cannot answer the user's query with the given context.</td>\n",
       "      <td>1861</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "                          display_name  failed  \\\n",
       "0  Context Contains Enough Information    True   \n",
       "1  Context Contains Enough Information    True   \n",
       "2  Context Contains Enough Information    True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                              grade_reason  \\\n",
       "0  The context provided does not contain sufficient information to answer the user's query about the capital of Greece. The context only mentions that Greece is often called the cradle of Western civilization, but it does not provide any specific information about the capital city.   \n",
       "1                      The context provided does not contain sufficient information to answer the user's query about the price of a Tesla Model 3. The context only mentions that Tesla Model 3 is a fully electric car, but it does not provide any specific information about its price.   \n",
       "2                                                                                                                           The context provided does not contain any information related to shooting stars. Therefore, the chatbot cannot answer the user's query with the given context.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     2183  gpt-3.5-turbo    failed           1.0  \n",
       "1     2078  gpt-3.5-turbo    failed           1.0  \n",
       "2     1861  gpt-3.5-turbo    failed           1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the context contains enough information to answer the user query provided\n",
    "ContextContainsEnoughInformation(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes.</td>\n",
       "      <td>1380</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes, so it passes the grading criteria.</td>\n",
       "      <td>1717</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes.</td>\n",
       "      <td>1562</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "  display_name  failed  \\\n",
       "0       Custom   False   \n",
       "1       Custom   False   \n",
       "2       Custom   False   \n",
       "\n",
       "                                                                    grade_reason  \\\n",
       "0                                     The response does not mention black holes.   \n",
       "1  The response does not mention black holes, so it passes the grading criteria.   \n",
       "2                                     The response does not mention black holes.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     1380  gpt-3.5-turbo    failed           0.0  \n",
       "1     1717  gpt-3.5-turbo    failed           0.0  \n",
       "2     1562  gpt-3.5-turbo    failed           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom evaluator\n",
    "# Checks if the response mentions black holes\n",
    "grading_criteria=\"If the response mentions black holes, then fail. Otherwise pass.\"\n",
    "CustomGrader(model=eval_model, grading_criteria=grading_criteria).run_batch(data=dataset).to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
