{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "from athina.evals import (\n",
    "    DoesResponseAnswerQuery,\n",
    "    ContextContainsEnoughInformation,\n",
    "    Faithfulness,\n",
    "    RagasContextRelevancy,\n",
    "    RagasAnswerRelevancy,\n",
    "    RagasContextPrecision,\n",
    "    RagasFaithfulness,\n",
    "    RagasContextRecall,\n",
    "    RagasAnswerSemanticSimilarity,\n",
    "    RagasAnswerCorrectness,\n",
    "    RagasHarmfulness,\n",
    "    RagasMaliciousness,\n",
    "    RagasCoherence,\n",
    "    RagasConciseness\n",
    ")\n",
    "\n",
    "from athina.evals import FunctionEvaluator\n",
    "from athina.loaders import RagLoader, ResponseLoader, RagasLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.interfaces.athina import AthinaFilters\n",
    "import pandas as pd\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_ragas = [\n",
    "    {\n",
    "        \"query\": \"Where is France and what is it's capital?\",\n",
    "        \"contexts\": [\"France is the country in europe known for delicious cuisine\", \"Tesla is an electric car\", \"Elephant is an animal\"],\n",
    "        \"response\": \"Tesla is an electric car\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Where is France and what is it's capital?\",\n",
    "        \"contexts\": [\"France is the country in europe known for delicious cuisine\", \"Paris is the capital of france\"],\n",
    "        \"response\": \"France is in western Europe and Paris is its capital\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_raw_data_ragas = RagasLoader().load_dict(raw_data_ragas)\n",
    "pd.DataFrame(dataset_raw_data_ragas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"query\": \"Where is France and what is it's capital?\",\n",
    "        \"contexts\": [\"France is the country in europe known for delicious cuisine\", \"Tesla is an electric car\", \"Elephant is an animal\"],\n",
    "        \"response\": \"Tesla is an electric car\",\n",
    "    }\n",
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasAnswerRelevancy(model=eval_model).run(**data).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasAnswerRelevancy(model=eval_model).run_batch(data=dataset_raw_data_ragas).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_ragas_with_expected_response = [\n",
    "    {\n",
    "        \"query\": \"Where is France and what is it's capital?\",\n",
    "        \"contexts\": [\"France is the country in europe known for delicious cuisine\", \"Tesla is an electric car\", \"Elephant is an animal\"],\n",
    "        \"response\": \"Tesla is an electric car\",\n",
    "        \"expected_response\": \"France is in europe. Paris is it's capital\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is Tesla? Who founded it?\",\n",
    "        \"contexts\": [\"Tesla is the electric car company. Tesla is registerd in United States\", \"Elon Musk founded it\"],\n",
    "        \"response\": \"France is in western Europe and Paris is its capital\",\n",
    "        \"expected_response\": \"Tesla is an electric car company. Elon Musk founded it.\"\n",
    "    },\n",
    "]\n",
    "dataset_raw_data_ragas_with_expected_response = RagasLoader().load_dict(raw_data_ragas_with_expected_response)\n",
    "pd.DataFrame(dataset_raw_data_ragas_with_expected_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasContextPrecision(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasContextRelevancy(model=eval_model).run_batch(data=dataset_raw_data_ragas).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasFaithfulness(model=eval_model).run_batch(data=dataset_raw_data_ragas).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasContextRecall(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasAnswerSemanticSimilarity(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasAnswerCorrectness(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasHarmfulness(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasMaliciousness(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasCoherence(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "RagasConciseness(model=eval_model).run_batch(data=dataset_raw_data_ragas_with_expected_response).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch dataset from list of dict objects\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of Greece?\",\n",
    "        \"context\": \"Greece is often called the cradle of Western civilization.\",\n",
    "        \"response\": \"Athens\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the price of a Tesla Model 3?\",\n",
    "        \"context\": \"Tesla Model 3 is a fully electric car.\",\n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a shooting star?\",\n",
    "        \"context\": \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\",\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"gpt-3.5-turbo\"\n",
    "ContextContainsEnoughInformation(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the LLM response answers the user query sufficiently\n",
    "eval_model = \"gpt-3.5-turbo\"\n",
    "DoesResponseAnswerQuery(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the LLM response is faithful to the information provided to it\n",
    "eval_model = \"gpt-3.5-turbo\"\n",
    "Faithfulness(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can run our function based evaluators as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from athina.evals import ContainsAny, Regex\n",
    "from athina.loaders import ResponseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_data = [ \n",
    "    { \n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval checks if the response contains any of the keywords\n",
    "ContainsAny(keywords=[\"star\"]).run_batch(data=dataset).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_data = [ \n",
    "    { \n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"Contact us at hello@athina.ai to get access to our LLM observability platform where you can run the tests you've defined here against your LLM responses in production.\",\n",
    "    }\n",
    "]\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval checks if the response matches the regex\n",
    "Regex(regex='([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)').run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsNone\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\n",
    "        \"response\": \"This text does not contain the specified keyword.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"This is a text without any specified search word.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsNone(keywords=[\"keyword\"]).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import Contains\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\n",
    "        \"response\": \"The keyword YC present in this text.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"This text does not contain the specified word.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "Contains(keyword=\"YC\").run_batch(data=dataset).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsAll\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"This text contains both keyword1 and keyword2.\"},\n",
    "    {\"response\": \"This text does not contain all specified keywords.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsAll(keywords=[\"keyword1\", \"keyword2\"]).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsJson\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": '{\"key\": \"value\"}'},\n",
    "    {\"response\": '{\"invalid : \"json\"}'},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsJson().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsEmail\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Contact us at contact@example.com.\"},\n",
    "    {\"response\": \"This text does not contain any email address.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsEmail().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import IsJson\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": '{\"key\": \"value\"}'},\n",
    "    {\"response\": 'invalid_json'},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "IsJson().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import IsEmail\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"john.doe@example.com\"},\n",
    "    {\"response\": \"invalid.email\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "IsEmail().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsLink\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"For more information, visit https://example.com.\"},\n",
    "    {\"response\": \"This text does not contain any link.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsLink().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ContainsValidLink\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Visit our official website at http://example.com.\"},\n",
    "    {\"response\": \"Visit our official website at https://exampleasdf.com\"},\n",
    "    {\"response\": \"This text does not contain any valid link.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ContainsValidLink().run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import NoInvalidLinks\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Visit our website at https://example.com.\"},\n",
    "    {\"response\": \"Visit our official website at https://exampleasdf.com\"},\n",
    "    {\"response\": \"This text does not contain any valid link.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "\n",
    "# Example calls\n",
    "NoInvalidLinks().run_batch(data=dataset).to_df()\n",
    "NoInvalidLinks().run_batch(data=dataset).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import ApiCall\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Response to be sent to the your own API based evaluator\"}\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "ApiCall(url=\"https://8e714940905f4022b43267e348b8a71.api.mockbin.io/\", payload={\"evaluator\": \"custom_api_based_evaluator\"}, headers={\"Authorization\": \"Bearer token\"}).run_batch(data=dataset).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import Equals\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"This is the expected response\"},\n",
    "    {\"response\": \"This is an unexpected response\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "Equals(expected_response=\"This is the expected response\").run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import StartsWith\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"The text starts with this substring.\"},\n",
    "    {\"response\": \"This text does not start with the specified substring.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "StartsWith(substring=\"The text starts with\").run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import EndsWith\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"The text ends with this substring.\"},\n",
    "    {\"response\": \"This text does not end with the specified substring.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "EndsWith(substring=\"with this substring.\").run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import LengthLessThan\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Short text\"},\n",
    "    {\"response\": \"This is a longer text.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "LengthLessThan(max_length=20).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athina.evals import LengthGreaterThan\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Short text\"},\n",
    "    {\"response\": \"This is a longer text.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = ResponseLoader().load_dict(raw_data)\n",
    "LengthGreaterThan(min_length=20).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The evaluators mentioned so far are ungrounded - meaning that they do not compare the response against any reference data. You can run the following evaluators to do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fci/Documents/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Short text</td>\n",
       "      <td>Another Short text</td>\n",
       "      <td>CosineSimilarity</td>\n",
       "      <td>None</td>\n",
       "      <td>Successfully calculated similarity score using CosineSimilarity</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a longer text.</td>\n",
       "      <td>This is a very different bigger string.</td>\n",
       "      <td>CosineSimilarity</td>\n",
       "      <td>None</td>\n",
       "      <td>Successfully calculated similarity score using CosineSimilarity</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.507093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 response                        expected_response  \\\n",
       "0              Short text                       Another Short text   \n",
       "1  This is a longer text.  This is a very different bigger string.   \n",
       "\n",
       "       display_name failed  \\\n",
       "0  CosineSimilarity   None   \n",
       "1  CosineSimilarity   None   \n",
       "\n",
       "                                                      grade_reason  runtime  \\\n",
       "0  Successfully calculated similarity score using CosineSimilarity        0   \n",
       "1  Successfully calculated similarity score using CosineSimilarity        0   \n",
       "\n",
       "  model  similarity_score  \n",
       "0  None          0.816497  \n",
       "1  None          0.507093  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AnswerSimilarity\n",
    "from athina.evals import AnswerSimilarity\n",
    "from athina.evals.grounded.similarity import CosineSimilarity, NormalisedLevenshteinSimilarity\n",
    "from athina.loaders import Loader\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\"response\": \"Short text\", \"expected_response\": \"Another Short text\"},\n",
    "    {\"response\": \"This is a longer text.\", \"expected_response\": \"This is a very different bigger string.\"},\n",
    "]\n",
    "\n",
    "# Load data into dataset\n",
    "dataset = Loader().load_dict(raw_data)\n",
    "# Cosine similarity\n",
    "AnswerSimilarity(comparator=CosineSimilarity()).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Short text</td>\n",
       "      <td>Another Short text</td>\n",
       "      <td>CosineSimilarity</td>\n",
       "      <td>False</td>\n",
       "      <td>Successfully calculated similarity score using CosineSimilarity</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a longer text.</td>\n",
       "      <td>This is a very different bigger string.</td>\n",
       "      <td>CosineSimilarity</td>\n",
       "      <td>True</td>\n",
       "      <td>Successfully calculated similarity score using CosineSimilarity</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.507093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 response                        expected_response  \\\n",
       "0              Short text                       Another Short text   \n",
       "1  This is a longer text.  This is a very different bigger string.   \n",
       "\n",
       "       display_name  failed  \\\n",
       "0  CosineSimilarity   False   \n",
       "1  CosineSimilarity    True   \n",
       "\n",
       "                                                      grade_reason  runtime  \\\n",
       "0  Successfully calculated similarity score using CosineSimilarity        0   \n",
       "1  Successfully calculated similarity score using CosineSimilarity        0   \n",
       "\n",
       "  model  similarity_score  \n",
       "0  None          0.816497  \n",
       "1  None          0.507093  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity with failure threshold\n",
    "AnswerSimilarity(comparator=CosineSimilarity(), failure_threshold = 0.8).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised Levenshtein similarity\n",
    "AnswerSimilarity(comparator=NormalisedLevenshteinSimilarity()).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContextSimilarity\n",
    "from athina.evals import ContextSimilarity\n",
    "from athina.evals.grounded.similarity import CosineSimilarity\n",
    "from athina.loaders import Loader\n",
    "\n",
    "# Example data\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"Where is France?\",\n",
    "        \"context\": \"France is the country in europe\",\n",
    "        \"response\": \"France is in western Europe\"\n",
    "    }\n",
    "]\n",
    "# Load data into dataset\n",
    "dataset = Loader().load_dict(raw_data)\n",
    "# Cosine similarity\n",
    "ContextSimilarity(comparator=CosineSimilarity()).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can define your own custom similarity comparator also\n",
    "class MyCustomSimilarity:\n",
    "    def compare(self, string1, string2):\n",
    "        set1 = set(string1)\n",
    "        set2 = set(string2)\n",
    "        common_characters = set1.intersection(set2)\n",
    "        score = sum(min(string1.count(char), string2.count(char)) for char in common_characters)\n",
    "        max_score = len(string1) + len(string2)\n",
    "        return score / max_score\n",
    "\n",
    "ContextSimilarity(comparator=MyCustomSimilarity()).run_batch(data=dataset).to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
