{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.llms.openai_service import OpenAiService\n",
    "from athina.evals import DoesResponseAnswerQuery, ContextContainsEnoughInformation, Faithfulness, CustomGrader\n",
    "from athina.loaders import RagLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.interfaces.athina import AthinaFilters\n",
    "import pandas as pd\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western c...</td>\n",
       "      <td>Athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed unde...</td>\n",
       "      <td>A shooting star is a meteor that burns up in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Greece is often called the cradle of Western c...   \n",
       "1             Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed unde...   \n",
       "\n",
       "                                            response  \n",
       "0                                             Athens  \n",
       "1  I cannot answer this question as prices vary f...  \n",
       "2  A shooting star is a meteor that burns up in t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batch dataset from list of dict objects\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of Greece?\",\n",
    "        \"context\": \"Greece is often called the cradle of Western civilization.\",\n",
    "        \"response\": \"Athens\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the price of a Tesla Model 3?\",\n",
    "        \"context\": \"Tesla Model 3 is a fully electric car.\",\n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a shooting star?\",\n",
    "        \"context\": \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\",\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response 'Athens' answers specifically what the user is asking about, which is the capital of Greece. It covers all aspects of the user's query and provides the correct answer.</td>\n",
       "      <td>1627</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response answers the user's query sufficiently. It acknowledges that the question is about the price of a Tesla Model 3 and explains that prices vary from country to country, which covers all aspects of the user's query.</td>\n",
       "      <td>1463</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Does Response Answer Query</td>\n",
       "      <td>False</td>\n",
       "      <td>The response answers the user's query specifically and covers all aspects of the user's query. It defines a shooting star as a meteor that burns up in the atmosphere, which is exactly what the user asked for.</td>\n",
       "      <td>2184</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "                 display_name  failed  \\\n",
       "0  Does Response Answer Query   False   \n",
       "1  Does Response Answer Query   False   \n",
       "2  Does Response Answer Query   False   \n",
       "\n",
       "                                                                                                                                                                                                                       grade_reason  \\\n",
       "0                                              The response 'Athens' answers specifically what the user is asking about, which is the capital of Greece. It covers all aspects of the user's query and provides the correct answer.   \n",
       "1  The response answers the user's query sufficiently. It acknowledges that the question is about the price of a Tesla Model 3 and explains that prices vary from country to country, which covers all aspects of the user's query.   \n",
       "2                  The response answers the user's query specifically and covers all aspects of the user's query. It defines a shooting star as a meteor that burns up in the atmosphere, which is exactly what the user asked for.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     1627  gpt-3.5-turbo    failed           0.0  \n",
       "1     1463  gpt-3.5-turbo    failed           0.0  \n",
       "2     2184  gpt-3.5-turbo    failed           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the LLM response answers the user query sufficiently\n",
    "eval_model = \"gpt-3.5-turbo\"\n",
    "DoesResponseAnswerQuery(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>True</td>\n",
       "      <td>The response 'Athens' cannot be inferred purely from the context. The context mentions that Greece is often called the cradle of Western civilization, but it does not directly state that Athens is the cradle of Western civilization.</td>\n",
       "      <td>1843</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>False</td>\n",
       "      <td>The response can be inferred from the context. The context states that the prices of Tesla Model 3 vary from country to country, and the response acknowledges this fact by stating that it cannot answer the question as prices vary. Therefore, the response is directly related to the information provided in the context.</td>\n",
       "      <td>1940</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>True</td>\n",
       "      <td>The response cannot be inferred from the provided context. The context describes black holes, while the response talks about shooting stars. There is no connection or mention of shooting stars in the context.</td>\n",
       "      <td>1534</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "   display_name  failed  \\\n",
       "0  Faithfulness    True   \n",
       "1  Faithfulness   False   \n",
       "2  Faithfulness    True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     grade_reason  \\\n",
       "0                                                                                        The response 'Athens' cannot be inferred purely from the context. The context mentions that Greece is often called the cradle of Western civilization, but it does not directly state that Athens is the cradle of Western civilization.   \n",
       "1  The response can be inferred from the context. The context states that the prices of Tesla Model 3 vary from country to country, and the response acknowledges this fact by stating that it cannot answer the question as prices vary. Therefore, the response is directly related to the information provided in the context.   \n",
       "2                                                                                                                The response cannot be inferred from the provided context. The context describes black holes, while the response talks about shooting stars. There is no connection or mention of shooting stars in the context.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     1843  gpt-3.5-turbo    failed           1.0  \n",
       "1     1940  gpt-3.5-turbo    failed           0.0  \n",
       "2     1534  gpt-3.5-turbo    failed           1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the LLM response is faithful to the information provided to it\n",
    "Faithfulness(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context does not contain sufficient information to answer the user's query about the capital of Greece. The context only provides information about Greece being called the cradle of Western civilization, but it does not directly mention the capital city. Therefore, the chatbot cannot answer the user's query with just the given context.</td>\n",
       "      <td>2623</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context does not contain sufficient information to answer the user's query about the price of a Tesla Model 3. The context only mentions that Tesla Model 3 is a fully electric car, but it does not provide any specific information about its price. Therefore, the chatbot cannot answer the user's query with just the given context.</td>\n",
       "      <td>1894</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Context Contains Enough Information</td>\n",
       "      <td>True</td>\n",
       "      <td>The context provided does not contain sufficient information to answer the user's query about shooting stars. The context only talks about black holes and their gravitational pull, which is unrelated to shooting stars. Therefore, the chatbot cannot answer the user's query with the given context.</td>\n",
       "      <td>1860</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "                          display_name  failed  \\\n",
       "0  Context Contains Enough Information    True   \n",
       "1  Context Contains Enough Information    True   \n",
       "2  Context Contains Enough Information    True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                            grade_reason  \\\n",
       "0  The context does not contain sufficient information to answer the user's query about the capital of Greece. The context only provides information about Greece being called the cradle of Western civilization, but it does not directly mention the capital city. Therefore, the chatbot cannot answer the user's query with just the given context.   \n",
       "1          The context does not contain sufficient information to answer the user's query about the price of a Tesla Model 3. The context only mentions that Tesla Model 3 is a fully electric car, but it does not provide any specific information about its price. Therefore, the chatbot cannot answer the user's query with just the given context.   \n",
       "2                                               The context provided does not contain sufficient information to answer the user's query about shooting stars. The context only talks about black holes and their gravitational pull, which is unrelated to shooting stars. Therefore, the chatbot cannot answer the user's query with the given context.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     2623  gpt-3.5-turbo    failed           1.0  \n",
       "1     1894  gpt-3.5-turbo    failed           1.0  \n",
       "2     1860  gpt-3.5-turbo    failed           1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks if the context contains enough information to answer the user query provided\n",
    "ContextContainsEnoughInformation(model=eval_model).run_batch(data=dataset).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>display_name</th>\n",
       "      <th>failed</th>\n",
       "      <th>grade_reason</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Greece?</td>\n",
       "      <td>Greece is often called the cradle of Western civilization.</td>\n",
       "      <td>Athens</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes.</td>\n",
       "      <td>1113</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the price of a Tesla Model 3?</td>\n",
       "      <td>Tesla Model 3 is a fully electric car.</td>\n",
       "      <td>I cannot answer this question as prices vary from country to country.</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes, so it passes the grading criteria.</td>\n",
       "      <td>941</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a shooting star?</td>\n",
       "      <td>Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.</td>\n",
       "      <td>A shooting star is a meteor that burns up in the atmosphere.</td>\n",
       "      <td>Custom</td>\n",
       "      <td>False</td>\n",
       "      <td>The response does not mention black holes.</td>\n",
       "      <td>850</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>failed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0         What is the capital of Greece?   \n",
       "1  What is the price of a Tesla Model 3?   \n",
       "2               What is a shooting star?   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0                                                                                              Greece is often called the cradle of Western civilization.   \n",
       "1                                                                                                                  Tesla Model 3 is a fully electric car.   \n",
       "2  Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.   \n",
       "\n",
       "                                                                response  \\\n",
       "0                                                                 Athens   \n",
       "1  I cannot answer this question as prices vary from country to country.   \n",
       "2           A shooting star is a meteor that burns up in the atmosphere.   \n",
       "\n",
       "  display_name  failed  \\\n",
       "0       Custom   False   \n",
       "1       Custom   False   \n",
       "2       Custom   False   \n",
       "\n",
       "                                                                    grade_reason  \\\n",
       "0                                     The response does not mention black holes.   \n",
       "1  The response does not mention black holes, so it passes the grading criteria.   \n",
       "2                                     The response does not mention black holes.   \n",
       "\n",
       "   runtime          model metric_id  metric_value  \n",
       "0     1113  gpt-3.5-turbo    failed           0.0  \n",
       "1      941  gpt-3.5-turbo    failed           0.0  \n",
       "2      850  gpt-3.5-turbo    failed           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom evaluator\n",
    "# Checks if the response mentions black holes\n",
    "grading_criteria=\"If the response mentions black holes, then fail. Otherwise pass.\"\n",
    "CustomGrader(model=eval_model, grading_criteria=grading_criteria).run_batch(data=dataset).to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
