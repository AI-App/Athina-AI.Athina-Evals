{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from athina.evals import DoesResponseAnswerQuery, ContextContainsEnoughInformation, Faithfulness, LlmEvaluator\n",
    "from athina.loaders import RagLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.runner.run import EvalRunner\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch dataset from list of dict objects\n",
    "raw_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of Greece?\",\n",
    "        \"context\": \"Greece is often called the cradle of Western civilization.\",\n",
    "        \"response\": \"Athens\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the price of a Tesla Model 3?\",\n",
    "        \"context\": \"Tesla Model 3 is a fully electric car.\",\n",
    "        \"response\": \"I cannot answer this question as prices vary from country to country.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a shooting star?\",\n",
    "        \"context\": \"Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.\",\n",
    "        \"response\": \"A shooting star is a meteor that burns up in the atmosphere.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'data_point': {'query': 'What is the capital of Greece?', 'context': 'Greece is often called the cradle of Western civilization.', 'response': 'Athens'}, 'eval_results': [{'failure': False, 'reason': \"The response directly answers the user's query by stating 'Athens,' which is the capital of Greece. It specifically addresses the question without providing unnecessary information.\", 'runtime': 2520.3652381896973, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': \"The response 'Athens' cannot be inferred from the provided context. The context only states that Greece is often called the cradle of Western civilization, but it does not specifically mention Athens or imply that Athens is the reason for this designation.\", 'runtime': 2289.492130279541, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': \"The context provided only mentions Greece as 'the cradle of Western civilization' and does not include any information about the capital of Greece. Therefore, the chatbot cannot answer the user's query about the capital of Greece using only the provided context.\", 'runtime': 2298.7492084503174, 'model': 'gpt-4-1106-preview'}]}, {'data_point': {'query': 'What is the price of a Tesla Model 3?', 'context': 'Tesla Model 3 is a fully electric car.', 'response': 'I cannot answer this question as prices vary from country to country.'}, 'eval_results': [{'failure': True, 'reason': \"The response does not answer the user's query sufficiently. It does not provide any specific price information or a range for the Tesla Model 3, and instead, it only mentions that prices vary by country without giving any further details or context.\", 'runtime': 2406.3491821289062, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': 'The response cannot be inferred from the provided context. The context only states that the Tesla Model 3 is a fully electric car, but it does not provide any information about the pricing of the car or how it varies from country to country.', 'runtime': 2504.6122074127197, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': \"The context provided only states that the Tesla Model 3 is a fully electric car, but it does not include any information about the price. Therefore, the chatbot cannot answer the user's query about the price of a Tesla Model 3 using only the given context.\", 'runtime': 2806.5600395202637, 'model': 'gpt-4-1106-preview'}]}, {'data_point': {'query': 'What is a shooting star?', 'context': 'Black holes are stars that have collapsed under their own gravity. They are so dense that nothing can escape their gravitational pull, not even light.', 'response': 'A shooting star is a meteor that burns up in the atmosphere.'}, 'eval_results': [{'failure': False, 'reason': \"The response directly answers the user's query by explaining that a shooting star is a meteor that burns up in the atmosphere, which is a succinct and accurate description of the phenomenon referred to as a shooting star.\", 'runtime': 2323.0879306793213, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': 'The response cannot be inferred from the provided context. The context discusses the nature of black holes, but it does not provide any information about shooting stars or meteors.', 'runtime': 1831.007957458496, 'model': 'gpt-4-1106-preview'}, {'failure': True, 'reason': \"The context provided defines black holes and does not contain information about shooting stars, which are actually meteors burning up in Earth's atmosphere. Therefore, the context does not enable the chatbot to answer the user's query about what a shooting star is.\", 'runtime': 2806.4820766448975, 'model': 'gpt-4-1106-preview'}]}], 'evals': [{'name': 'draq', 'display_name': 'Does Response Answer Query'}, {'name': 'faithfulness', 'display_name': 'Faithfulness'}, {'name': 'ccei', 'display_name': 'Context Contains Enough Information'}], 'total_runtime': 21786.70597076416, 'passed_evals': 2, 'failed_evals': 7, 'total_evals': 9, 'total_datapoints': 3}\n"
     ]
    }
   ],
   "source": [
    "# Run the eval suite\n",
    "eval_model = \"gpt-4-1106-preview\"\n",
    "eval_suite = [\n",
    "    DoesResponseAnswerQuery(model=eval_model),\n",
    "    Faithfulness(model=eval_model),\n",
    "    ContextContainsEnoughInformation(model=eval_model),\n",
    "    LlmEvaluator(model=eval_model, grading_criteria=\"If response contains profanity, then fail. Otherwise, pass.\")\n",
    "]\n",
    "\n",
    "# Run the evaluation suite\n",
    "batch_eval_result = EvalRunner.run_batch(evals=eval_suite, dataset=dataset)\n",
    "\n",
    "print(batch_eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
